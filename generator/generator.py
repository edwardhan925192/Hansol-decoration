# -*- coding: utf-8 -*-
"""generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19btg8oDkGW37emLZrYv7Yae8dDgTyvAz
"""

def generate_text_directly(model, tokenizer, question, context="", max_length=512):
    prompt_text = f"{context}\n\n질문: {question}\n답변:" if context else f"질문: {question}\n답변:"
    inputs = tokenizer(prompt_text, return_tensors="pt", padding=True, truncation=True, max_length=max_length).to(model.device)
    output_tokens = model.generate(**inputs, max_length=max_length, num_return_sequences=1)
    answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
    return answer